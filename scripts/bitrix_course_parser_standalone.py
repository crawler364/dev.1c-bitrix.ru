#!/usr/bin/env python3
"""
–ü–∞—Ä—Å–µ—Ä –∫—É—Ä—Å–æ–≤ Bitrix (–ê–≤—Ç–æ–Ω–æ–º–Ω–∞—è –≤–µ—Ä—Å–∏—è)
–ü–∞—Ä—Å–µ—Ä –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∫—É—Ä—Å–æ–≤ —Å —Å–∞–π—Ç–∞ dev.1c-bitrix.ru
–í–µ—Ä—Å–∏—è –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ Python

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python bitrix_course_parser_standalone.py [--limit N] [--output DIR]
    
–ê—Ä–≥—É–º–µ–Ω—Ç—ã:
    --limit N    : –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∫–∞—á–∏–≤–∞–µ–º—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π)
    --output DIR : –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: ./course_data)
"""

import urllib.request
import urllib.parse
import html.parser
import os
import argparse
import time
import re
import json
import gzip
import html
from datetime import datetime

# –ò–º–ø–æ—Ä—Ç —Ñ—É–Ω–∫—Ü–∏–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–∞—Ä—Ç—ã –∫—É—Ä—Å–æ–≤
try:
    from course_map_generator import scan_courses_directory, generate_course_map
except ImportError:
    # –ï—Å–ª–∏ –º–æ–¥—É–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫–∏
    def scan_courses_directory(data_dir):
        return []
    def generate_course_map(courses, output_file):
        pass


class SimpleHTMLParser(html.parser.HTMLParser):
    def __init__(self):
        super().__init__()
        self.links = []
        self.title = ""
        self.in_title = False
        
    def handle_starttag(self, tag, attrs):
        if tag == 'title':
            self.in_title = True
        elif tag == 'a':
            href = None
            for attr_name, attr_value in attrs:
                if attr_name == 'href':
                    href = attr_value
            # –£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ —É—Ä–æ–∫–∏
            if href:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Å—ã–ª–∫–∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å —É—Ä–æ–∫–∞–º–∏
                if ('lesson' in href.lower() or 
                    'LESSON_ID' in href or 
                    'CHAPTER_ID' in href or
                    ('/learning/course/' in href and ('LESSON_ID=' in href or 'CHAPTER_ID=' in href))):
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º CSS –∏ –¥—Ä—É–≥–∏–µ –Ω–µ –æ—Ç–Ω–æ—Å—è—â–∏–µ—Å—è –∫ –∫–æ–Ω—Ç–µ–Ω—Ç—É —Å—Å—ã–ª–∫–∏
                    if not href.endswith('.css') and not href.endswith('.js'):
                        self.links.append(href)
    
    def handle_data(self, data):
        if self.in_title:
            self.title += data.strip()
    
    def handle_endtag(self, tag):
        if tag == 'title':
            self.in_title = False

class TextExtractorParser(html.parser.HTMLParser):
    def __init__(self):
        super().__init__()
        self.text_content = []
        self.in_script = False
        self.in_style = False
        
    def handle_starttag(self, tag, attrs):
        if tag in ['script', 'style']:
            setattr(self, f'in_{tag}', True)
    
    def handle_data(self, data):
        if not self.in_script and not self.in_style:
            text = data.strip()
            if text:
                self.text_content.append(text)
    
    def handle_endtag(self, tag):
        if tag in ['script', 'style']:
            setattr(self, f'in_{tag}', False)
    
    def get_text_content(self):
        cleaned_lines = []
        for line in self.text_content:
            line = line.strip()
            if line and len(line) > 1:
                cleaned_lines.append(line)
        return '\n'.join(cleaned_lines)


class MarkdownExtractorParser(html.parser.HTMLParser):
    def __init__(self):
        super().__init__()
        self.content_parts = []
        self.headers = []
        self.text_content = []
        self.current_tag = None
        self.in_script = False
        self.in_style = False
        self.in_courses_right_side = False
        self.div_nesting_level = 0  # –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º –≥–ª—É–±–∏–Ω—É –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏ div
        
    def handle_starttag(self, tag, attrs):
        self.current_tag = tag
        
        if tag in ['script', 'style']:
            setattr(self, f'in_{tag}', True)
        elif tag == 'div':
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª–∞—Å—Å–∞ courses-right-side
            has_courses_right_side = False
            for attr_name, attr_value in attrs:
                if attr_name == 'class' and 'courses-right-side' in attr_value:
                    has_courses_right_side = True
                    break
            
            if has_courses_right_side:
                self.in_courses_right_side = True
                self.div_nesting_level = 1  # –ù–∞—á–∏–Ω–∞–µ–º –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç—å —Å —É—Ä–æ–≤–Ω—è 1
            elif self.in_courses_right_side:
                # –ú—ã –≤–Ω—É—Ç—Ä–∏ courses-right-side –∏ –Ω–∞—à–ª–∏ –µ—â–µ –æ–¥–∏–Ω div
                self.div_nesting_level += 1
        elif tag == 'a' and not self.in_script and not self.in_style:
            href = None
            for attr_name, attr_value in attrs:
                if attr_name == 'href':
                    href = attr_value
            if href and href.startswith('http'):
                self.current_link_href = href
                self.current_link_text = ""
    
    def handle_data(self, data):
        if self.in_script or self.in_style:
            return
            
        text = data.strip()
        if not text:
            return
            
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ courses-right-side –±–ª–æ–∫–∞
        if not self.in_courses_right_side:
            return
            
        if self.current_tag in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
            level = int(self.current_tag[1]) + 1  # –°–º–µ—â–∞–µ–º —É—Ä–æ–≤–µ–Ω—å –Ω–∞ 1
            self.headers.append(f"{'#' * level} {text}")
        elif hasattr(self, 'current_link_href'):
            self.current_link_text += text
        else:
            self.text_content.append(text)
    
    def handle_endtag(self, tag):
        if tag in ['script', 'style']:
            setattr(self, f'in_{tag}', False)
        elif tag == 'div' and self.in_courses_right_side:
            # –£–º–µ–Ω—å—à–∞–µ–º —É—Ä–æ–≤–µ–Ω—å –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ div –≤–Ω—É—Ç—Ä–∏ courses-right-side
            self.div_nesting_level -= 1
            # –í—ã—Ö–æ–¥–∏–º –∏–∑ courses-right-side —Ç–æ–ª—å–∫–æ –∫–æ–≥–¥–∞ –∑–∞–∫—Ä—ã–≤–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π div (—É—Ä–æ–≤–µ–Ω—å 0)
            if self.div_nesting_level == 0:
                self.in_courses_right_side = False
        elif tag == 'a' and hasattr(self, 'current_link_href'):
            if self.current_link_text:
                # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç —Å—Å—ã–ª–∫–∏ –∫–∞–∫ –∂–∏—Ä–Ω—ã–π —Ç–µ–∫—Å—Ç, –±–µ–∑ URL
                bold_text = f"**{self.current_link_text.strip()}**"
                self.text_content.append(bold_text)
            delattr(self, 'current_link_href')
            if hasattr(self, 'current_link_text'):
                delattr(self, 'current_link_text')
        
        self.current_tag = None
    
    def get_markdown_content(self, url, title="–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"):
        md_lines = []
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        md_lines.append(f"# {title}")
        md_lines.append("")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        md_lines.append("## –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ")
        md_lines.append("")
        md_lines.append(f"- **URL:** {url}")
        md_lines.append("")
        md_lines.append("---")
        md_lines.append("")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
        md_lines.append("## –°–æ–¥–µ—Ä–∂–∏–º–æ–µ")
        md_lines.append("")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
        if self.headers:
            md_lines.extend(self.headers)
            md_lines.append("")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç
        if self.text_content:
            md_lines.append("### –û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç")
            md_lines.append("")
            cleaned_lines = []
            for line in self.text_content:
                line = line.strip()
                if line and len(line) > 1:
                    # –ò–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
                    is_header = False
                    for header in self.headers:
                        if line in header:
                            is_header = True
                            break
                    if not is_header:
                        cleaned_lines.append(line)
            md_lines.extend(cleaned_lines)
        
        return '\n'.join(md_lines)
    
    def has_courses_right_side_content(self):
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –±—ã–ª –ª–∏ –Ω–∞–π–¥–µ–Ω –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ –±–ª–æ–∫–∞ courses-right-side
        
        Returns:
            bool: True –µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ courses-right-side, False –∏–Ω–∞—á–µ
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∏–ª–∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        return bool(self.headers or self.text_content)


class BitrixCourseParser:
    def __init__(self, start_url, output_dir="./course_data", page_limit=None, timeout=0.5):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞
        
        Args:
            start_url: –ù–∞—á–∞–ª—å–Ω—ã–π URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
            output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
            page_limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
            timeout: –¢–∞–π–º–∞—É—Ç –º–µ–∂–¥—É —Å–∫–∞—á–∏–≤–∞–Ω–∏—è–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        """
        self.start_url = start_url
        self.output_dir = output_dir
        self.page_limit = page_limit
        self.timeout = timeout
        self.downloaded_pages = 0
        self.visited_urls = set()
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –≤—ã–≤–æ–¥–∞
        os.makedirs(output_dir, exist_ok=True)
        
    def sanitize_filename(self, filename):
        """–û—á–∏—Å—Ç–∫–∞ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –æ—Ç –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
        filename = filename.strip().strip('.')
        return filename[:200]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
    
    def get_page_content(self, url):
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        
        Args:
            url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            
        Returns:
            (parser, content) –∏–ª–∏ (None, None) –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        """
        try:
            print(f"–ó–∞–≥—Ä—É–∂–∞–µ–º: {url}")
            
            # –°–æ–∑–¥–∞–µ–º –∑–∞–ø—Ä–æ—Å —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –±—Ä–∞—É–∑–µ—Ä–∞
            req = urllib.request.Request(
                url,
                headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8',
                    'Accept-Encoding': 'gzip, deflate',
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1',
                }
            )
            
            with urllib.request.urlopen(req, timeout=30) as response:
                # –ß–∏—Ç–∞–µ–º –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
                raw_content = response.read()
            
                # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞–∫–æ–≤–∞—Ç—å –µ—Å–ª–∏ —Å–∂–∞—Ç–æ gzip
                try:
                    if response.headers.get('Content-Encoding') == 'gzip':
                        content = gzip.decompress(raw_content).decode('utf-8', errors='ignore')
                    else:
                        # –ü—ã—Ç–∞–µ–º—Å—è gzip –≤ –ª—é–±–æ–º —Å–ª—É—á–∞–µ –µ—Å–ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
                        try:
                            content = gzip.decompress(raw_content).decode('utf-8', errors='ignore')
                        except:
                            content = raw_content.decode('utf-8', errors='ignore')
                except Exception as e:
                    content = raw_content.decode('utf-8', errors='ignore')
            
                # –î–µ–∫–æ–¥–∏—Ä—É–µ–º HTML —Å—É—â–Ω–æ—Å—Ç–∏
                content = html.unescape(content)
            
                # –ü–∞—Ä—Å–∏–º HTML
                parser = SimpleHTMLParser()
                parser.feed(content)
            
                return parser, content
                
        except urllib.error.HTTPError as e:
            print(f"HTTP –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}: {e.code} - {e.reason}")
            return None, None
        except urllib.error.URLError as e:
            print(f"–û—à–∏–±–∫–∞ URL –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}: {e.reason}")
            return None, None
        except Exception as e:
            print(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {url}: {e}")
            return None, None
    
    def extract_course_info(self, parser):
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫—É—Ä—Å–µ
        
        Args:
            parser: SimpleHTMLParser object
            
        Returns:
            dict —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫—É—Ä—Å–µ
        """
        course_info = {
            'title': parser.title or '–ö—É—Ä—Å Bitrix',
            'description': '',
            'lessons': [],
            'metadata': {}
        }
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏
        print(f"–û—Ç–ª–∞–¥–∫–∞: –Ω–∞–π–¥–µ–Ω–æ {len(parser.links)} —Å—ã—Ä—ã—Ö —Å—Å—ã–ª–æ–∫")
        lesson_links = []
        base_url = f"{urllib.parse.urlparse(self.start_url).scheme}://{urllib.parse.urlparse(self.start_url).netloc}"
        
        for href in parser.links:
            if href:
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ URL –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ
                if href.startswith('/'):
                    full_url = base_url + href
                elif href.startswith('http'):
                    full_url = href
                else:
                    full_url = urllib.parse.urljoin(self.start_url, href)
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ URL
                parsed_url = urllib.parse.urlparse(full_url)
                query_params = urllib.parse.parse_qs(parsed_url.query)
                
                title_parts = []
                if 'LESSON_ID' in query_params:
                    title_parts.append(f"–£—Ä–æ–∫ {query_params['LESSON_ID'][0]}")
                elif 'CHAPTER_ID' in query_params:
                    title_parts.append(f"–ì–ª–∞–≤–∞ {query_params['CHAPTER_ID'][0]}")
                else:
                    title_parts.append("–£—Ä–æ–∫")
                
                title = " ".join(title_parts)
                
                if full_url not in [l['url'] for l in lesson_links]:
                    lesson_links.append({
                        'title': title,
                        'url': full_url
                    })
        
        course_info['lessons'] = lesson_links
        return course_info
    
    def save_page_content(self, url, parser, content, page_info=None):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ MD
        
        Args:
            url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            parser: SimpleHTMLParser object
            content: –°—ã—Ä–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ HTML
            page_info: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        """
        try:
            # –°–æ–∑–¥–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ URL
            parsed_url = urllib.parse.urlparse(url)
            query_params = urllib.parse.parse_qs(parsed_url.query)
            
            filename_parts = []
            if 'COURSE_ID' in query_params:
                filename_parts.append(f"course_{query_params['COURSE_ID'][0]}")
            if 'LESSON_ID' in query_params:
                filename_parts.append(f"lesson_{query_params['LESSON_ID'][0]}")
            
            if not filename_parts:
                filename_parts.append(f"page_{self.downloaded_pages + 1}")
            
            if page_info and page_info.get('title'):
                safe_title = self.sanitize_filename(page_info['title'])
                filename_parts.append(safe_title)
            
            base_filename = "_".join(filename_parts)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown
            md_parser = MarkdownExtractorParser()
            md_parser.feed(content)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –±—ã–ª –ª–∏ –Ω–∞–π–¥–µ–Ω –±–ª–æ–∫ courses-right-side —Å –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
            if not md_parser.has_courses_right_side_content():
                print(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: –±–ª–æ–∫ courses-right-side –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –ø—É—Å—Ç")
                return
            
            title = page_info.get('title', parser.title) if page_info else parser.title
            if not title:
                title = "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
            
            md_content = md_parser.get_markdown_content(url, title)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø–∞–ø–∫—É —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –Ω–∞–∑–≤–∞–Ω–∏—é –∫—É—Ä—Å–∞
            if 'COURSE_ID' in query_params:
                course_id = query_params['COURSE_ID'][0]
                course_subdir = os.path.join(self.output_dir, f"course_{course_id}")
                os.makedirs(course_subdir, exist_ok=True)
                
                course_md_filename = os.path.join(course_subdir, f"{base_filename}.md")
                with open(course_md_filename, 'w', encoding='utf-8') as f:
                    f.write(md_content)
            else:
                # –ï—Å–ª–∏ ID –∫—É—Ä—Å–∞ –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø–∞–ø–∫—É data/course
                course_subdir = os.path.join(self.output_dir, "course")
                os.makedirs(course_subdir, exist_ok=True)
                
                course_md_filename = os.path.join(course_subdir, f"{base_filename}.md")
                with open(course_md_filename, 'w', encoding='utf-8') as f:
                    f.write(md_content)
            
            print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ MD: {base_filename}")
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: {e}")
    
    def parse_course(self):
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫—É—Ä—Å–∞
        """
        print(f"–ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –∫—É—Ä—Å–∞: {self.start_url}")
        print(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {self.output_dir}")
        if self.page_limit:
            print(f"–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü: {self.page_limit}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞—á–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
        parser, content = self.get_page_content(self.start_url)
        if not parser:
            print("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∞—á–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É")
            return
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫—É—Ä—Å–µ
        print(f"–û—Ç–ª–∞–¥–∫–∞: –Ω–∞–π–¥–µ–Ω–æ {len(parser.links)} —Å—ã—Ä—ã—Ö —Å—Å—ã–ª–æ–∫")
        if parser.links:
            print(f"–ü–µ—Ä–≤—ã–µ 5 —Å—Å—ã–ª–æ–∫: {parser.links[:5]}")
        
        course_info = self.extract_course_info(parser)
        print(f"–ù–∞–π–¥–µ–Ω –∫—É—Ä—Å: {course_info['title']}")
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Ä–æ–∫–æ–≤: {len(course_info['lessons'])}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø–∞–ø–∫—É —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –Ω–∞–∑–≤–∞–Ω–∏—é –∫—É—Ä—Å–∞
        parsed_url = urllib.parse.urlparse(self.start_url)
        query_params = urllib.parse.parse_qs(parsed_url.query)
        
        if 'COURSE_ID' in query_params:
            course_id = query_params['COURSE_ID'][0]
            course_subdir = os.path.join(self.output_dir, f"course_{course_id}")
            os.makedirs(course_subdir, exist_ok=True)
            
            course_info_subfile = os.path.join(course_subdir, 'course_info.json')
            with open(course_info_subfile, 'w', encoding='utf-8') as f:
                json.dump(course_info, f, ensure_ascii=False, indent=2)
        else:
            # –ï—Å–ª–∏ ID –∫—É—Ä—Å–∞ –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø–∞–ø–∫—É data/course
            course_subdir = os.path.join(self.output_dir, "course")
            os.makedirs(course_subdir, exist_ok=True)
            
            course_info_subfile = os.path.join(course_subdir, 'course_info.json')
            with open(course_info_subfile, 'w', encoding='utf-8') as f:
                json.dump(course_info, f, ensure_ascii=False, indent=2)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞—á–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
        self.save_page_content(
            self.start_url, 
            parser, 
            content, 
            {'title': course_info['title'] or 'course_index'}
        )
        self.downloaded_pages += 1
        self.visited_urls.add(self.start_url)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —É—Ä–æ–∫–∏
        for i, lesson in enumerate(course_info['lessons']):
            if self.page_limit and self.downloaded_pages >= self.page_limit:
                print(f"–î–æ—Å—Ç–∏–≥–Ω—É—Ç–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –≤ {self.page_limit} —Å—Ç—Ä–∞–Ω–∏—Ü")
                break
            
            if lesson['url'] in self.visited_urls:
                continue
            
            print(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —É—Ä–æ–∫ {i+1}/{len(course_info['lessons'])}: {lesson['title']}")
            
            # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            time.sleep(self.timeout)
            
            lesson_parser, lesson_content = self.get_page_content(lesson['url'])
            if lesson_parser and lesson_content:
                self.save_page_content(
                    lesson['url'],
                    lesson_parser,
                    lesson_content,
                    {'title': lesson['title']}
                )
                self.downloaded_pages += 1
                self.visited_urls.add(lesson['url'])
        
        print(f"–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –°–∫–∞—á–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {self.downloaded_pages}")
        print(f"–§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {os.path.abspath(self.output_dir)}")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–∞—Ä—Ç—É –∫—É—Ä—Å–æ–≤ –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞
        print("üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–∞—Ä—Ç—ã –∫—É—Ä—Å–æ–≤...")
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞ (–Ω–∞ —É—Ä–æ–≤–µ–Ω—å –≤—ã—à–µ output_dir)
            project_root = os.path.dirname(os.path.abspath(self.output_dir))
            data_dir = os.path.abspath(self.output_dir)
            output_file = os.path.join(project_root, 'COURSES_MAP.md')
            
            # –°–∫–∞–Ω–∏—Ä—É–µ–º –∫—É—Ä—Å—ã –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–∞—Ä—Ç—É
            courses = scan_courses_directory(data_dir)
            if courses:
                generate_course_map(courses, output_file)
                print(f"‚úÖ –ö–∞—Ä—Ç–∞ –∫—É—Ä—Å–æ–≤ –æ–±–Ω–æ–≤–ª–µ–Ω–∞: {output_file}")
            else:
                print("‚ö†Ô∏è  –ö—É—Ä—Å—ã –¥–ª—è –∫–∞—Ä—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–∞—Ä—Ç—ã –∫—É—Ä—Å–æ–≤: {e}")


def main():
    parser = argparse.ArgumentParser(description='–ü–∞—Ä—Å–µ—Ä –∫—É—Ä—Å–æ–≤ Bitrix (Standalone –≤–µ—Ä—Å–∏—è)')
    parser.add_argument(
        '--url',
        default='https://dev.1c-bitrix.ru/learning/course/index.php?COURSE_ID=43&INDEX=Y',
        help='URL –∫—É—Ä—Å–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞'
    )
    parser.add_argument(
        '--limit',
        type=int,
        help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è'
    )
    parser.add_argument(
        '--output',
        default='./course_data',
        help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤'
    )
    parser.add_argument(
        '--timeout',
        type=float,
        default=0.5,
        help='–¢–∞–π–º–∞—É—Ç –º–µ–∂–¥—É —Å–∫–∞—á–∏–≤–∞–Ω–∏—è–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.5)'
    )
    
    args = parser.parse_args()
    
    # –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—Å–µ—Ä
    course_parser = BitrixCourseParser(
        start_url=args.url,
        output_dir=args.output,
        page_limit=args.limit,
        timeout=args.timeout
    )
    
    try:
        course_parser.parse_course()
    except KeyboardInterrupt:
        print("\n–ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")


if __name__ == "__main__":
    main()
